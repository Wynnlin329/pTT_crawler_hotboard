{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pymysql\n",
    "\n",
    "#創建table\n",
    "def create_mysql_table(board):\n",
    "    db = pymysql.connect(\"mysql\",\"root\",\"123456\",\"pttDB\",charset='utf8mb4',)\n",
    "    cursor = db.cursor()\n",
    "    sql =\"\"\"create table if not exists {}(articleID varchar(16) primary key, \n",
    "            authorId varchar(14) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            authorName varchar(14) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci ,\n",
    "            category varchar(4) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            title varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            publishedTime varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            content text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            canonicalUrl varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "            createdTime varchar(30),\n",
    "            updateTime varchar(30),\n",
    "            commentId longtext,\n",
    "            commentContent longtext,\n",
    "            commentTime longtext)\n",
    "        \"\"\".format(board)\n",
    "    \n",
    "    cursor.execute(sql)\n",
    "    db.commit()\n",
    "    db.close()\n",
    "    \n",
    "#插入資料\n",
    "def insert_data(board,articleID , authorId ,authorName , category ,title ,publishedTime ,content ,canonicalUrl ,createdTime ,updateTime ,commentId ,commentContent ,commentTime,g,page_count):\n",
    "    db = pymysql.connect(\"mysql\",\"root\",\"123456\",\"pttDB\",charset='utf8mb4',use_unicode = True )\n",
    "    cursor = db.cursor()\n",
    "    sql = \"\"\"insert into {}(articleID,authorId,authorName,category,title,publishedTime,content,\n",
    "             canonicalUrl,createdTime,updateTime,commentId,commentContent,commentTime) \n",
    "             values('{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}')\"\"\".format(\n",
    "             board, str(articleID) ,str(authorId) ,str(authorName) , str(category) ,str(title) ,str(publishedTime),str(content) ,str(canonicalUrl) ,str(createdTime) ,\n",
    "             str(updateTime) ,pymysql.escape_string(str(commentId)) ,pymysql.escape_string(str(commentContent)),pymysql.escape_string(str(commentTime)))\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        db.commit()\n",
    "    except:\n",
    "        db.rollback()\n",
    "        db.close()\n",
    "    print('存好第',page_count,'頁  第',g,'篇資料')\n",
    "\n",
    "\n",
    "#時間格式修改(資料建立時間與資料更新時間)\n",
    "def time_convert(createdTime):\n",
    "    import time \n",
    "    try:\n",
    "        tempTime = time.strptime(createdTime,'%a %b %d %H:%M:%S %Y')\n",
    "        resTime = time.strftime('%Y-%m-%dT%H:%M:%S.020+00:00',tempTime)\n",
    "    except ValueError :\n",
    "        tempTime = time.strptime(createdTime,' %m/%d/%Y %H:%M:%S')\n",
    "        resTime = time.strftime('%Y-%m-%dT%H:%M:%S.020+00:00',tempTime)\n",
    "    return resTime\n",
    "\n",
    "#時間格式修改(貼文和推文時間改時間戳)\n",
    "def time_mktime(publishedTime):\n",
    "    import time\n",
    "    try:\n",
    "        resTime = str(round(time.mktime(time.strptime(publishedTime,'%a %b %d %H:%M:%S %Y')),))+\"000\"\n",
    "    except Exception:\n",
    "        resTime = str(round(time.mktime(time.strptime(publishedTime,' %Y %m/%d %H:%M')),))\n",
    "    return resTime\n",
    "\n",
    "#比較文章時間的格式修改\n",
    "def compare_mktime(publishedTime):\n",
    "    import time\n",
    "    try:\n",
    "        resTime = str(round(time.mktime(time.strptime(publishedTime,'%a %b %d %H:%M:%S %Y')),))\n",
    "    except Exception:\n",
    "        resTime = str(round(time.mktime(time.strptime(publishedTime,' %Y %m/%d %H:%M')),))\n",
    "    return resTime\n",
    "\n",
    "#推文的時間格式修改\n",
    "def commentTime_mktime(commentTime_year):\n",
    "    import time\n",
    "    try:\n",
    "        resTime = str(round(time.mktime(time.strptime(commentTime_year,' %Y %m/%d %H:%M:%S')),))\n",
    "    except Exception:\n",
    "        resTime = str(round(time.mktime(time.strptime(commentTime_year,' %Y %m/%d')),))\n",
    "    return resTime\n",
    "\n",
    "#頁數日期修改\n",
    "def last_mktime(start_date_tmp):\n",
    "    import time\n",
    "    resTime = str(round(time.mktime(time.strptime(start_date_tmp,'%Y-%m-%d')),))\n",
    "    return resTime\n",
    "\n",
    "#文章內文\n",
    "def content(artical_tmp):       \n",
    "    artical_content = artical_tmp[0].text.split('--')[0].replace('\\n','').replace(' ','').replace('\\t','')  \n",
    "    print(artical_content)\n",
    "    return artical_content\n",
    "\n",
    "#完整推文    \n",
    "def push_content(content_count):\n",
    "    comment_word = []\n",
    "    for count in range(0,len(content_count)):\n",
    "        commentContent = content_count[count].text.replace(\":\",\"\")\n",
    "        comment_word.append(commentContent)\n",
    "    print(comment_word)\n",
    "    return comment_word\n",
    "\n",
    "#推文作者    \n",
    "def push_commentID(commentID_count):\n",
    "    comment_user = []\n",
    "    for count in range(0,len(commentID_count)):\n",
    "        commentID = commentID_count[count].text\n",
    "        comment_user.append(commentID)\n",
    "    print(comment_user)\n",
    "    return comment_user\n",
    "\n",
    "#推文時間\n",
    "def push_commentTime(artical_count,time_year): \n",
    "    push_time = []\n",
    "    for count in range(0,len(artical_count)):\n",
    "        if len(artical_count[count].text) <= 28 :       #判斷推文是否有時分秒，若有正常抓\n",
    "            commentTime_tmp = artical_count[count].text.replace('\\n','')[-11:]  #推文時間\n",
    "            commentTime_year = '{} '.format(time_year)+commentTime_tmp  \n",
    "            commentTime = time_mktime(commentTime_year)\n",
    "            \n",
    "        else:\n",
    "            import time\n",
    "            result = time.ctime() #若格式不一樣沒有時分秒  就調整格式\n",
    "            hours = result[-13:-5]\n",
    "            commentTime_tmp = artical_count[count].text.replace('\\n','')[-5:]  #推文時間\n",
    "            commentTime_year = '{} '.format(time_year)+commentTime_tmp+\" \"+hours  \n",
    "            commentTime = commentTime_mktime(commentTime_year)\n",
    "        push_time.append(commentTime)\n",
    "    print(push_time)\n",
    "    return push_time\n",
    "\n",
    "#擷取總頁數\n",
    "def allpage_number(board):                                           \n",
    "    urlpage = 'https://www.ptt.cc/bbs/{}/index.html'.format(board)\n",
    "    res = requests.get(urlpage,headers = headers, cookies = {'over18':'1'})\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    allpageurl = soup.select('.btn.wide')[1]['href']\n",
    "    start_index = allpageurl.find('index')\n",
    "    end_index = allpageurl.find('.html')\n",
    "    page_number = eval(\"%s +1\" %(allpageurl[start_index + 5 : end_index]))\n",
    "    print(\"總頁數\",page_number)\n",
    "    return page_number\n",
    "\n",
    "\n",
    "\"================================================================================================\"\n",
    "'''\n",
    "[爬下所設定的所有分類看板]\n",
    "取出設定的其中一個看板，接著取出看板的所有頁數，再將每一頁的每篇文章的標題與網址取出\n",
    "接著取得文章分類，文章分類有少部分格式不一樣，所以列為其他。\n",
    "再來爬取貼文時間 文章內容 推文內容 推文作者 推文時間等等 \n",
    "\n",
    "一開始會針對分類看板建立一個table，抓到下個看板會再建立另外一個table\n",
    "每抓一篇資料會儲存一次。\n",
    "\n",
    "'''\n",
    "\n",
    "#設定抓所有看板的文章\n",
    "def all_board(class_board):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36'\n",
    "    headers = {'User-Agent':user_agent}\n",
    "    for board in class_board:\n",
    "        create_mysql_table(board)                       #建立mysql的分類table\n",
    "        page_number = allpage_number(board)\n",
    "        for page_count in range(page_number,0,-1):      #抓取分類看板的所有頁數，再由最新抓回來\n",
    "            time_year = 0\n",
    "            url_allpage = 'https://www.ptt.cc/bbs/{}/index{}.html'.format(board,page_count)\n",
    "            res = requests.get(url_allpage,headers = headers, cookies = {'over18':'1'})\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            title_url = soup.select('div[class=\"title\"] a')\n",
    "            print('第',page_count,'頁')\n",
    "            for g,i in enumerate(title_url) :            #取出分頁中各篇文章的網址\n",
    "                try:\n",
    "                    sort_tmp = i.text\n",
    "                    sort = re.findall(r'\\[\\w\\w',sort_tmp)[0].replace('[','')\n",
    "                    print('文章分類為:'+ sort)             #文章分類\n",
    "                except Exception :\n",
    "                    sort = '其他'\n",
    "                    print('文章分類為: 其他')\n",
    "                try:\n",
    "                    title = i.text.replace(' ','').split(']')[1]     #文章標題\n",
    "                    print(title)\n",
    "                except Exception :\n",
    "                    title = i.text\n",
    "                    print(title)\n",
    "                    \n",
    "                article_id_tmp = i['href']      #文章編號\n",
    "                end_index = article_id_tmp.find('.html')\n",
    "                article_id = article_id_tmp[-23:end_index].replace(\".\",\"\")\n",
    "                print(article_id)\n",
    "                \n",
    "                \n",
    "                canonicalUrl = 'https://www.ptt.cc'+i['href']    #標準網址\n",
    "                print(canonicalUrl)\n",
    "                authorID_tmp = soup.select('div[class=\"author\"] ')            \n",
    "                authorID = authorID_tmp[g].text  #作者編號\n",
    "                print('作者標號: '+authorID)\n",
    "                url1= canonicalUrl\n",
    "                res1 = requests.get(url1,headers = headers, cookies = {'over18':'1'})\n",
    "                soup1 = BeautifulSoup(res1.text, 'html.parser')\n",
    "                author = soup1.select('span[class=\"article-meta-value\"]')\n",
    "                try:\n",
    "                    authorName_tmp = author[0].text.replace(')','').split('(')\n",
    "                    authorName = authorName_tmp[1]\n",
    "                    print('作者暱稱 '+authorName)      #作者暱稱\n",
    "                except Exception :\n",
    "                    authorName = '沒有暱稱'\n",
    "                    print(authorName)\n",
    "                try:\n",
    "                    time = soup1.select('span[class=\"article-meta-value\"]')\n",
    "                    someTime_tmp = time[3].text\n",
    "                    time_year = someTime_tmp[-5:]\n",
    "                    print(\"年分: \"+time_year)\n",
    "                    publishedTime = time_mktime(someTime_tmp)                   #貼文時間\n",
    "\n",
    "                    print(\"貼文時間: \"+publishedTime)\n",
    "                except Exception :\n",
    "                    publishedTime = '版主公告 沒有時間'\n",
    "                    print(publishedTime)\n",
    "\n",
    "                artical_tmp = soup1.select('div[id=\"main-container\"]')\n",
    "                artical_content = content(artical_tmp)         #文章內文\n",
    "                content_count = soup1.select('div [class=\"push\"] [class=\"f3 push-content\"]')\n",
    "                comment_word = push_content(content_count)     #推文內容\n",
    "                commentID_count = soup1.select('div [class=\"push\"] [class=\"f3 hl push-userid\"]')\n",
    "                comment_user = push_commentID(commentID_count) #推文作者\n",
    "                artical_count = soup1.select('div [class=\"push\"] [class=\"push-ipdatetime\"]')\n",
    "                try:\n",
    "                    if time_year != 0:\n",
    "                        push_time = push_commentTime(artical_count,time_year)  #推文時間\n",
    "                    else:\n",
    "                        updateTime_tmp11 = soup1.select('span[class=\"f2\"]')\n",
    "                        updateTime = updateTime_tmp11[3]\n",
    "                        updateTime_convert = updateTime.text.replace('\\n','').split(',')[1]\n",
    "                        time_year = updateTime_convert[-13:-8]\n",
    "\n",
    "                        push_time = push_commentTime(artical_count,time_year)   #推文時間\n",
    "                except ValueError:\n",
    "                    push_time = \"沒有推文時間\"\n",
    "                    print(\"沒有推文時間\")\n",
    "                \n",
    "                \n",
    "                import time\n",
    "                createdTime = time_convert(time.ctime())\n",
    "                updateTime = time_convert(time.ctime())\n",
    "                \n",
    "              \n",
    "                \n",
    "                insert_data(board,article_id , authorID ,authorName , sort ,   #將資料存入mysql\n",
    "                            title ,publishedTime ,artical_content ,canonicalUrl ,createdTime ,\n",
    "                            updateTime ,comment_user ,comment_word ,push_time,g,page_count)\n",
    "\n",
    "                \n",
    "\"================================================================================================\"\n",
    "\n",
    "'''\n",
    "[指定欲擷取的貼文發布日期區間]\n",
    "從最舊的文章開始搜尋，每一頁都抓當頁最新那篇文章的時間來與我要抓的起始時間比對，\n",
    "如果當頁最新的那篇文章大於我要抓的起始時間，就會開始讀取當頁第一篇文章，再去\n",
    "比較是不是介於我的起始時間與結束時間之中，如果符合再開始抓，避免在同一頁裡有\n",
    "不符合時間區間的文章。\n",
    "\n",
    "'''\n",
    "\n",
    " #抓設定範圍內的文章\n",
    "def range_search(start_date,end_date,class_board):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36'\n",
    "    headers = {'User-Agent':user_agent}\n",
    "    for board in class_board:\n",
    "        page_number = allpage_number(board)\n",
    "        for page_count in range(1,page_number):    #判斷每一頁的最後一篇文章\n",
    "            url= 'https://www.ptt.cc/bbs/NBA/index{}.html'.format(page_count+1)\n",
    "            print(\"第\",page_count,\"頁\")\n",
    "            res = requests.get(url,headers = headers, cookies = {'over18':'1'})    #請求分類看板首頁\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            last = soup.select('div [class=\"r-ent\"] ')\n",
    "            tmp_last = 'https://www.ptt.cc'+last[len(last)-1].a[\"href\"]    #取出當前頁最後一篇文章網址\n",
    "            print(tmp_last)\n",
    "            try:\n",
    "                res2 = requests.get(tmp_last,headers = headers, cookies = {'over18':'1'}) \n",
    "                soup_list = BeautifulSoup(res2.text, 'html.parser')\n",
    "                time2 = soup_list.select('span[class=\"article-meta-value\"]')\n",
    "                someTime_tmp2 = time2[3].text\n",
    "                lastTime2 = compare_mktime(someTime_tmp2)  #當前頁面 最後一篇文章的貼文時間\n",
    "                print(\"最後一篇貼文時間: \"+lastTime2)\n",
    "            except Exception :\n",
    "                print('貼文時間關閉')\n",
    "\n",
    "            if start_date <= lastTime2 :    #判斷每一頁的最後一篇文章的時間\n",
    "                for compare_count in range(0,len(last)):\n",
    "                    compare_url = 'https://www.ptt.cc'+last[compare_count].a['href']   #取出整頁文章網址\n",
    "                    print('第',compare_count,'篇文章的URL :'+compare_url)\n",
    "                    res3 = requests.get(compare_url,headers = headers, cookies = {'over18':'1'})\n",
    "                    soup_content = BeautifulSoup(res3.text, 'html.parser')\n",
    "                    try:\n",
    "                        time3 = soup_content.select('span[class=\"article-meta-value\"]')\n",
    "                        someTime_tmp3 = time3[3].text\n",
    "                        lastTime3 = compare_mktime(someTime_tmp3)   #抓出第一篇文章時間\n",
    "                        print('第一篇文章的時間'+lastTime3)\n",
    "                    except Exception:\n",
    "                        continue           #若文章被刪除，則換下一篇\n",
    "                    if  lastTime3 >= start_date and lastTime3 <= end_date:      #第一篇文章時間大於起始時間就抓整頁文章\n",
    "                        \n",
    "                        time_year = 0\n",
    "                        #for g,i in enumerate(title_url) :\n",
    "                        try:\n",
    "                            sort_tmp = last[compare_count].text\n",
    "                            sort = re.findall(r'\\[\\w\\w',sort_tmp)[0].replace('[','')\n",
    "                            print('文章分類為:'+ sort)                                     #文章分類\n",
    "                        except Exception :\n",
    "                            print('文章分類為: 其他')\n",
    "                            \n",
    "                        \n",
    "                        lastt = soup.select('div[class=\"title\"] a')\n",
    "                        try:\n",
    "                            title = lastt[compare_count].text.replace(' ','').split(']')[1]   #文章標題\n",
    "                            print(title)\n",
    "                        except Exception :\n",
    "                            title = lastt[compare_count].text\n",
    "                            print(title)\n",
    "                        article_id_tmp = lastt[compare_count]['href']                         #文章編號\n",
    "                        end_index = article_id_tmp.find('.html')\n",
    "                        article_id = article_id_tmp[-23:end_index].replace(\".\",\"\")\n",
    "                        print(article_id)\n",
    "                        canonicalUrl = 'https://www.ptt.cc'+lastt[compare_count]['href']      #標準網址\n",
    "                        print(canonicalUrl)\n",
    "                        \n",
    "                        authorID_tmp = soup.select('div[class=\"author\"] ')            \n",
    "                        authorID = authorID_tmp[compare_count].text                           #作者編號\n",
    "                        print('作者標號 : '+authorID)\n",
    "                        author = soup_content.select('span[class=\"article-meta-value\"]')\n",
    "                        try:\n",
    "                            authorName_tmp = author[0].text.replace(')','').split('(')\n",
    "                            authorName = authorName_tmp[1]\n",
    "                            print('作者暱稱 '+authorName)                                  #作者暱稱\n",
    "                        except Exception :\n",
    "                            authorName = '沒有暱稱'\n",
    "                            print(authorName)\n",
    "                        try:\n",
    "                            time = soup_list.select('span[class=\"article-meta-value\"]')\n",
    "                            someTime_tmp = time[3].text\n",
    "                            time_year = someTime_tmp[-5:]\n",
    "                            print(\"年分: \"+time_year)\n",
    "                            publishedTime = time_mktime(someTime_tmp)                     #貼文時間\n",
    "\n",
    "                            print(\"貼文時間: \"+publishedTime)\n",
    "                        except Exception :\n",
    "                            publishedTime = '版主公告 沒有時間'\n",
    "                            print(publishedTime)\n",
    "     \n",
    "                        artical_tmp = soup_content.select('div[id=\"main-container\"]')\n",
    "                        artical_content = content(artical_tmp)                      #文章內文\n",
    "                        content_count = soup_content.select('div [class=\"push\"] [class=\"f3 push-content\"]')\n",
    "                        comment_word = push_content(content_count)                  #推文內容\n",
    "                        commentID_count = soup_content.select('div [class=\"push\"] [class=\"f3 hl push-userid\"]')\n",
    "                        comment_user = push_commentID(commentID_count)              #推文作者\n",
    "                        artical_count = soup_content.select('div [class=\"push\"] [class=\"push-ipdatetime\"]')\n",
    "                        try:\n",
    "                            if time_year != 0:\n",
    "                                push_time = push_commentTime(artical_count,time_year)#推文時間\n",
    "                                print('time正確')\n",
    "                            else:\n",
    "                                updateTime_tmp11 = soup_content.select('span[class=\"f2\"]')\n",
    "                                try:\n",
    "                                    updateTime = updateTime_tmp11[3]\n",
    "                                    updateTime_convert = updateTime.text.replace('\\n','').split(',')[1]\n",
    "                                    time_year = updateTime_convert[-13:-8]                            \n",
    "                                    push_time = push_commentTime(artical_count,time_year)#推文時間\n",
    "                                except Exception:\n",
    "                                    print('沒有推文時間')\n",
    "                        except ValueError:\n",
    "                            print(\"沒有推文時間\")\n",
    "                    elif lastTime3 < end_date:                             #判斷\n",
    "                        print('沒到起始時間')\n",
    "\n",
    "                    else :\n",
    "                        print('資料抓取完畢return')\n",
    "                        return                                             #抓完時間內的文章後跳出\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "'==============================================================================================='\n",
    "            \n",
    "if __name__ =='__main__':\n",
    "    \n",
    "    db = pymysql.connect(\"mysql\",\"root\",\"123456\",\"\", )       #與mysql連線並創建資料庫\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"create database if not exists pttDB\")\n",
    "    db = pymysql.connect(\"mysql\",\"root\",\"123456\",\"pttDB\",charset='utf8mb4' )\n",
    "    cursor.execute(\"use pttDB\")\n",
    "    db.commit()\n",
    "    db.close()\n",
    "\n",
    "    start_date_tmp = input(str('起始時間 格式為2020-01-01 : '))#要抓的起始時間\n",
    "    end_date_tmp   = input(str('結束時間 格式為2020-01-01 : '))#要抓的結束時間\n",
    "    #如果兩個都沒輸入就直接全抓\n",
    "    \n",
    "    if start_date_tmp == '' and end_date_tmp == '' :\n",
    "        print('---------------------全部都抓-----------------')\n",
    "        class_board = ['nCoV2019','NBA','Joke','Gossiping'] #可設定要抓哪幾個版\n",
    "        df = all_board(class_board)\n",
    "    else:\n",
    "        try:\n",
    "            start_date = last_mktime(start_date_tmp)\n",
    "            end_date = last_mktime(end_date_tmp)\n",
    "            print(start_date)\n",
    "            print(end_date)                     \n",
    "            class_board = ['NBA']  # 未來可增加抓多個版的功能\n",
    "            print('--------------------抓區間------------------')\n",
    "            range_search(start_date,end_date,class_board) \n",
    "        except Exception :\n",
    "            print('輸入不正確')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可印出區間內所有日期\n",
    "'''import datetime\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "published_time = datetime.now()\n",
    "a = '2019-07-08'\n",
    "startdate = parse(\"{}\".format(a))\n",
    "enddate = parse(\"2020-6-8\")'''\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
